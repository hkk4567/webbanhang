# @title model train recommend product (k·∫øt n·ªëi MySQL)
# ==============================================================================
# QUY TR√åNH HO√ÄN CH·ªàNH: TINH CH·ªàNH V√Ä ƒê√ÅNH GI√Å M√î H√åNH LAI (HYBRID MODEL)
# ==============================================================================
import pandas as pd
import numpy as np
import time
from sqlalchemy import create_engine
from sklearn.model_selection import train_test_split
from sklearn.decomposition import TruncatedSVD
from sklearn.preprocessing import MinMaxScaler

# ------------------------------------------------------------------------------
# ### <<< THAY ƒê·ªîI >>> ###
# B∆Ø·ªöC 1: K·∫æT N·ªêI V√Ä T·∫¢I D·ªÆ LI·ªÜU T·ª™ MYSQL
# ------------------------------------------------------------------------------
print("--- B∆Ø·ªöC 1: K·∫æT N·ªêI V√Ä T·∫¢I D·ªÆ LI·ªÜU T·ª™ MYSQL ---")

# --- Th√¥ng tin k·∫øt n·ªëi CSDL ---
db_user = 'root'
db_password = ''
db_host = 'localhost'
db_port = '3306'
db_name = 'websellproduct'

# T·∫°o engine k·∫øt n·ªëi
try:
    db_uri = f"mysql+mysqlconnector://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}"
    engine = create_engine(db_uri)
    print("‚úÖ K·∫øt n·ªëi th√†nh c√¥ng ƒë·∫øn MySQL.")
except Exception as e:
    print(f"‚ùå L·ªói k·∫øt n·ªëi CSDL: {e}")
    exit()

# --- T·∫£i d·ªØ li·ªáu t·ª´ c√°c b·∫£ng ---
try:
    # 1. T·∫£i b·∫£ng s·∫£n ph·∫©m (JOIN `products` v√† `categories`)
    sql_products = """
    SELECT
        p.id AS product_id,
        p.name AS product_name,
        c.name AS category
    FROM
        products AS p
    LEFT JOIN
        categories AS c ON p.category_id = c.id;
    """
    products_df = pd.read_sql(sql_products, engine)
    products_df['category'] = products_df['category'].fillna('Ch∆∞a ph√¢n lo·∫°i')
    print(f"‚úÖ ƒê√£ t·∫£i th√†nh c√¥ng {len(products_df)} s·∫£n ph·∫©m.")

    # 2. T·∫£i d·ªØ li·ªáu t∆∞∆°ng t√°c (JOIN `order_items` v√† `orders`)
    # ƒê√¢y s·∫Ω l√† ngu·ªìn d·ªØ li·ªáu ch√≠nh c·ªßa ch√∫ng ta (ratings_df)
    sql_interactions = """
    SELECT
        o.user_id,
        oi.product_id,
        oi.quantity
    FROM
        order_items AS oi
    JOIN
        orders AS o ON oi.order_id = o.id;
    """
    ratings_df = pd.read_sql(sql_interactions, engine)
    print(f"‚úÖ ƒê√£ t·∫£i {len(ratings_df)} d√≤ng d·ªØ li·ªáu t∆∞∆°ng t√°c.")

    if ratings_df.empty:
        print("‚ùå C·∫£nh b√°o: Kh√¥ng c√≥ d·ªØ li·ªáu t∆∞∆°ng t√°c. Kh√¥ng th·ªÉ ti·∫øp t·ª•c hu·∫•n luy·ªán.")
        exit()

except Exception as e:
    print(f"‚ùå L·ªói khi t·∫£i d·ªØ li·ªáu t·ª´ CSDL: {e}")
    exit()

# ==============================================================================
# C√ÅC B∆Ø·ªöC C√íN L·∫†I GI·ªÆ NGUY√äN
# ==============================================================================

# ------------------------------------------------------------------------------
# B∆Ø·ªöC 2: CHIA D·ªÆ LI·ªÜU TH√ÄNH 3 T·∫¨P (60-20-20)
# ------------------------------------------------------------------------------
print("\n--- B∆Ø·ªöC 2: CHIA D·ªÆ LI·ªÜU TH√ÄNH 3 T·∫¨P (60-20-20) ---")
# ƒê·∫£m b·∫£o c√≥ ƒë·ªß d·ªØ li·ªáu ƒë·ªÉ chia
if len(ratings_df) < 10:
    print("‚ùå L·ªói: D·ªØ li·ªáu qu√° √≠t ƒë·ªÉ chia th√†nh c√°c t·∫≠p train/validation/test.")
    exit()
    
train_val_df, test_df = train_test_split(ratings_df, test_size=0.2, random_state=42)
train_df, val_df = train_test_split(train_val_df, test_size=0.25, random_state=42) # 0.25 * 0.8 = 0.2

print(f"K√≠ch th∆∞·ªõc t·∫≠p Train:      {len(train_df)} (~{len(train_df)/len(ratings_df):.0%})")
print(f"K√≠ch th∆∞·ªõc t·∫≠p Validation: {len(val_df)} (~{len(val_df)/len(ratings_df):.0%})")
print(f"K√≠ch th∆∞·ªõc t·∫≠p Test:        {len(test_df)} (~{len(test_df)/len(ratings_df):.0%})")

# ------------------------------------------------------------------------------
# B∆Ø·ªöC 3: CHU·∫®N B·ªä MA TR·∫¨N V√Ä D·ªÆ LI·ªÜU ƒê√ÅNH GI√Å
# ------------------------------------------------------------------------------
print("\n--- B∆Ø·ªöC 3: CHU·∫®N B·ªä MA TR·∫¨N V√Ä D·ªÆ LI·ªÜU ƒê√ÅNH GI√Å ---")
# T·∫°o ma tr·∫≠n user-item t·ª´ t·∫≠p train
train_user_item_matrix = train_df.groupby(['user_id', 'product_id'])['quantity'].sum().reset_index().pivot_table(index='user_id', columns='product_id', values='quantity', fill_value=0)

# Th√™m c√°c s·∫£n ph·∫©m/user c√≥ th·ªÉ b·ªã thi·∫øu v√†o ma tr·∫≠n train ƒë·ªÉ ƒë·∫£m b·∫£o k√≠ch th∆∞·ªõc nh·∫•t qu√°n
all_users = ratings_df['user_id'].unique()
all_products = products_df['product_id'].unique()

missing_users = set(all_users) - set(train_user_item_matrix.index)
for user in missing_users:
    train_user_item_matrix.loc[user] = 0

missing_products = set(all_products) - set(train_user_item_matrix.columns)
for prod in missing_products:
    train_user_item_matrix[prod] = 0

train_user_item_matrix = train_user_item_matrix.sort_index(axis=0).sort_index(axis=1)

# D·ªØ li·ªáu ƒë·ªÉ ki·ªÉm ƒë·ªãnh
val_user_items = val_df.groupby('user_id')['product_id'].apply(set).to_dict()

# D·ªØ li·ªáu ƒë·ªÉ ki·ªÉm th·ª≠ cu·ªëi c√πng
test_user_items = test_df.groupby('user_id')['product_id'].apply(set).to_dict()
print("‚úÖ ƒê√£ chu·∫©n b·ªã xong ma tr·∫≠n train v√† c√°c t·∫≠p d·ªØ li·ªáu ƒë√°nh gi√°.")

# ------------------------------------------------------------------------------
# B∆Ø·ªöC 3.5: CHU·∫®N B·ªä CHO M√î H√åNH L·ªåC D·ª∞A TR√äN N·ªòI DUNG (CBF)
# ------------------------------------------------------------------------------
print("\n--- B∆Ø·ªöC 3.5: CHU·∫®N B·ªä CHO M√î H√åNH L·ªåC D·ª∞A TR√äN N·ªòI DUNG (CBF) ---")
df_product_features = pd.get_dummies(products_df.set_index('product_id')['category'])
# ƒê·∫£m b·∫£o c√°c c·ªôt/h√†ng kh·ªõp nhau
product_features_aligned = df_product_features.reindex(train_user_item_matrix.columns).fillna(0)

user_profiles_train = np.dot(train_user_item_matrix, product_features_aligned)
cbf_scores_train = np.dot(user_profiles_train, product_features_aligned.T)
df_cbf_scores_train = pd.DataFrame(cbf_scores_train, index=train_user_item_matrix.index, columns=train_user_item_matrix.columns)
print("‚úÖ ƒê√£ t·∫°o ma tr·∫≠n ƒëi·ªÉm CBF d·ª±a tr√™n t·∫≠p train.")

# ------------------------------------------------------------------------------
# B∆Ø·ªöC 4: TINH CH·ªàNH SI√äU THAM S·ªê CHO M√î H√åNH LAI
# ------------------------------------------------------------------------------
print("\n--- B∆Ø·ªöC 4: TINH CH·ªàNH SI√äU THAM S·ªê (k, alpha) TR√äN T·∫¨P VALIDATION ---")
start_time_tuning = time.time()

# ƒêi·ªÅu ch·ªânh k_values ƒë·ªÉ kh√¥ng l·ªõn h∆°n s·ªë chi·ªÅu c·ªßa ma tr·∫≠n
max_k = min(train_user_item_matrix.shape) - 1
k_values_to_try = [k for k in [10, 20, 30, 40] if k <= max_k]
if not k_values_to_try:
    k_values_to_try = [max_k] # √çt nh·∫•t ph·∫£i th·ª≠ m·ªôt gi√° tr·ªã k

alpha_values_to_try = [0.2, 0.5, 0.8]
results_val = []
k_for_recommendations = 10
scaler = MinMaxScaler()

for k_value in k_values_to_try:
    # ... (Ph·∫ßn c√≤n l·∫°i c·ªßa code ƒë∆∞·ª£c gi·ªØ nguy√™n)
    # 1. Hu·∫•n luy·ªán m√¥ h√¨nh CF (SVD)
    svd_model_tune = TruncatedSVD(n_components=k_value, random_state=42)
    matrix_p_tune = svd_model_tune.fit_transform(train_user_item_matrix)
    matrix_q_tune = svd_model_tune.components_
    predicted_ratings_cf = np.dot(matrix_p_tune, matrix_q_tune)
    df_predicted_cf = pd.DataFrame(predicted_ratings_cf, index=train_user_item_matrix.index, columns=train_user_item_matrix.columns)

    # 2. Chu·∫©n h√≥a ƒëi·ªÉm s·ªë
    df_cf_normalized = pd.DataFrame(scaler.fit_transform(df_predicted_cf), index=df_predicted_cf.index, columns=df_predicted_cf.columns)
    df_cbf_normalized = pd.DataFrame(scaler.fit_transform(df_cbf_scores_train), index=df_cbf_scores_train.index, columns=df_cbf_scores_train.columns)

    for alpha_value in alpha_values_to_try:
        # 3. K·∫øt h·ª£p ƒëi·ªÉm s·ªë
        df_hybrid_scores = alpha_value * df_cf_normalized + (1 - alpha_value) * df_cbf_normalized

        # 4. ƒê√°nh gi√° tr√™n t·∫≠p Validation
        all_precisions_val = []
        all_recalls_val = []

        for user_id, true_items in val_user_items.items():
            if user_id in df_hybrid_scores.index:
                predicted_scores = df_hybrid_scores.loc[user_id]
                original_train_scores = train_user_item_matrix.loc[user_id]
                unbought_items_scores = predicted_scores[original_train_scores == 0]
                top_k_recs = set(unbought_items_scores.sort_values(ascending=False).head(k_for_recommendations).index)

                hits = top_k_recs.intersection(true_items)
                precision = len(hits) / k_for_recommendations if k_for_recommendations > 0 else 0
                recall = len(hits) / len(true_items) if len(true_items) > 0 else 0
                all_precisions_val.append(precision)
                all_recalls_val.append(recall)

        mean_precision_val = np.mean(all_precisions_val) if all_precisions_val else 0
        mean_recall_val = np.mean(all_recalls_val) if all_recalls_val else 0
        results_val.append({'k': k_value, 'alpha': alpha_value, 'precision': mean_precision_val, 'recall': mean_recall_val})
        print(f"k = {k_value:2d}, alpha = {alpha_value:.1f} | Val Precision: {mean_precision_val:.4f} | Val Recall: {mean_recall_val:.4f}")

end_time_tuning = time.time()
print(f"‚úÖ Tinh ch·ªânh tham s·ªë ho√†n t·∫•t. Th·ªùi gian: {end_time_tuning - start_time_tuning:.2f} gi√¢y.")

# ------------------------------------------------------------------------------
# B∆Ø·ªöC 5: CH·ªåN (k, alpha) T·ªêT NH·∫§T V√Ä ƒê√ÅNH GI√Å CU·ªêI C√ôNG
# ------------------------------------------------------------------------------
if not results_val:
    print("‚ùå Kh√¥ng c√≥ k·∫øt qu·∫£ n√†o ƒë·ªÉ ƒë√°nh gi√°. D·ª´ng ch∆∞∆°ng tr√¨nh.")
    exit()

print("\n--- B∆Ø·ªöC 5: ƒê√ÅNH GI√Å CU·ªêI C√ôNG TR√äN T·∫¨P TEST ---")
results_val_df = pd.DataFrame(results_val)
best_params_row = results_val_df.loc[results_val_df['precision'].idxmax()]
best_k = int(best_params_row['k'])
best_alpha = best_params_row['alpha']
print(f"üèÜ Tham s·ªë t·ªët nh·∫•t t√¨m ƒë∆∞·ª£c t·ª´ t·∫≠p Validation: k = {best_k}, alpha = {best_alpha}")

# Hu·∫•n luy·ªán l·∫°i m√¥ h√¨nh cu·ªëi c√πng tr√™n to√†n b·ªô t·∫≠p train+validation
print(f"Hu·∫•n luy·ªán l·∫°i m√¥ h√¨nh cu·ªëi c√πng v·ªõi k={best_k}, alpha={best_alpha} tr√™n Train+Validation set...")
# T·∫°o ma tr·∫≠n cu·ªëi c√πng t·ª´ train+val
final_train_val_matrix = train_val_df.groupby(['user_id', 'product_id'])['quantity'].sum().reset_index().pivot_table(index='user_id', columns='product_id', values='quantity', fill_value=0)
# CƒÉn ch·ªânh l·∫°i ma tr·∫≠n cu·ªëi c√πng
final_train_val_matrix = final_train_val_matrix.reindex(index=all_users, columns=all_products, fill_value=0).sort_index(axis=0).sort_index(axis=1)


# 1. M√¥ h√¨nh CF cu·ªëi c√πng
final_model_cf = TruncatedSVD(n_components=best_k, random_state=42)
final_p = final_model_cf.fit_transform(final_train_val_matrix)
final_q = final_model_cf.components_
final_predicted_cf = pd.DataFrame(np.dot(final_p, final_q), index=final_train_val_matrix.index, columns=final_train_val_matrix.columns)

# 2. M√¥ h√¨nh CBF cu·ªëi c√πng
final_product_features_aligned = df_product_features.reindex(final_train_val_matrix.columns).fillna(0)
final_user_profiles = np.dot(final_train_val_matrix, final_product_features_aligned)
final_predicted_cbf = pd.DataFrame(np.dot(final_user_profiles, final_product_features_aligned.T), index=final_train_val_matrix.index, columns=final_train_val_matrix.columns)

# 3. Chu·∫©n h√≥a v√† k·∫øt h·ª£p
final_cf_norm = pd.DataFrame(scaler.fit_transform(final_predicted_cf), index=final_predicted_cf.index, columns=final_predicted_cf.columns)
final_cbf_norm = pd.DataFrame(scaler.fit_transform(final_predicted_cbf), index=final_cbf_aligned.index, columns=final_cbf_aligned.columns)
final_hybrid_scores = best_alpha * final_cf_norm + (1 - best_alpha) * final_cbf_norm

# 4. ƒê√°nh gi√° tr√™n t·∫≠p TEST
all_precisions_test = []
all_recalls_test = []
for user_id, true_items in test_user_items.items():
    if user_id in final_hybrid_scores.index:
        predicted_scores = final_hybrid_scores.loc[user_id]
        original_scores = final_train_val_matrix.loc[user_id]
        unbought_items_scores = predicted_scores[original_scores == 0]

        top_k_recs = set(unbought_items_scores.sort_values(ascending=False).head(k_for_recommendations).index)

        hits = top_k_recs.intersection(true_items)
        precision = len(hits) / k_for_recommendations if k_for_recommendations > 0 else 0
        recall = len(hits) / len(true_items) if len(true_items) > 0 else 0
        all_precisions_test.append(precision)
        all_recalls_test.append(recall)

final_precision = np.mean(all_precisions_test) if all_precisions_test else 0
final_recall = np.mean(all_recalls_test) if all_recalls_test else 0

print("\n--- K·∫æT QU·∫¢ CU·ªêI C√ôNG, KH√ÅCH QUAN TR√äN T·∫¨P TEST ---")
print(f"üìä Final Precision@{k_for_recommendations} (v·ªõi k={best_k}, alpha={best_alpha}): {final_precision:.4f}")
print(f"üìä Final Recall@{k_for_recommendations}    (v·ªõi k={best_k}, alpha={best_alpha}): {final_recall:.4f}")